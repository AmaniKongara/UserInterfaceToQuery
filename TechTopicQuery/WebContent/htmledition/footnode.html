
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>

<!-- Mirrored from nlp.stanford.edu/IR-book/html/htmledition/footnode.html by HTTrack Website Copier/3.x [XR&CO'2013], Sat, 08 Feb 2014 19:40:13 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<HEAD>
<TITLE>Footnotes</TITLE>
<META NAME="description" CONTENT="Footnotes">
<META NAME="keywords" CONTENT="irbook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="irbook-2.html">

<LINK REL="previous" HREF="index-1.html">
<LINK REL="up" HREF="irbook.html">
</HEAD>

<BODY >

<DL>
<DT><A NAME="foot814">...
email.</A><A
 HREF="boolean-retrieval-1.html#tex2html4"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In modern parlance, the word ``search'' has tended to replace
    ``(information) retrieval''; the term ``search'' is quite ambiguous,
    but in context we use the two synonymously.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot866">... it.</A><A
 HREF="an-example-information-retrieval-problem-1.html#tex2html5"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Formally, we take the transpose of the
  matrix to be able to get the terms as column vectors.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot1536">... retrieval.</A><A
 HREF="an-example-information-retrieval-problem-1.html#tex2html7"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Some information
  retrieval researchers prefer the term inverted
    file, but expressions like
  index construction and index compression
are much more common than inverted file construction
and inverted file compression. For consistency, we
use (inverted) index throughout this book.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot985">... .</A><A
 HREF="an-example-information-retrieval-problem-1.html#tex2html8"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In a (non-positional) inverted index, a posting is just a document ID, but it is inherently associated with a term, via the postings list it is placed on; sometimes we will also talk of a (term, docID) pair as a posting.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot1546">... merged.</A><A
 HREF="a-first-take-at-building-an-inverted-index-1.html#tex2html9"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Unix users can note that these steps are similar to use of
   the sort and then uniq commands.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot1552">... collection.</A><A
 HREF="processing-boolean-queries-1.html#tex2html12"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>The notation <IMG
 WIDTH="36" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img63.png"
 ALT="$\Theta(\cdot)$"> is used to express an asymptotically
    tight bound on the complexity of an algorithm.  Informally, this is
    often written as <IMG
 WIDTH="35" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img64.png"
 ALT="$O(\cdot)$">, but this notation really expresses an
    asymptotic upper bound, which need not be tight
    (<A
 HREF="bibliography-1.html#cormen90algorithms">Cormen et&nbsp;al., 1990</A>).

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot2853">... ,</A><A
 HREF="obtaining-the-character-sequence-in-a-document-1.html#tex2html15"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>A classifier is a function that takes objects of some sort
    and assigns them to one of a number of distinct classes (see Chapter <A HREF="text-classification-and-naive-bayes-1.html#ch:classification">13</A> ).  Usually
    classification is done by machine learning methods such as probabilistic models,
    but it can also be done by hand-written rules.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot2855">... .</A><A
 HREF="tokenization-1.html#tex2html16"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>That is, as defined here, tokens that are
  not indexed (stop words) <A NAME="2099"></A>are not terms, and if multiple
  tokens are collapsed together via normalization, they are
  indexed as one term, under the normalized form.
  However, we later relax this definition when discussing classification and
  clustering in nbayeslsi, where there is no index. In these
  chapters, we drop the requirement of inclusion in the dictionary.
  A <I>term</I> means a normalized word.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot2856">... query.</A><A
 HREF="tokenization-1.html#tex2html17"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>For the free text case, this is straightforward.  The Boolean case is
    more complex: this tokenization may produce multiple terms from one 
    query word.  This can be handled by combining the terms with an AND
    or as a phrase query phrasequery.
    It is harder for a system to handle the opposite case where the user
    entered as two terms something that was tokenized together in 
    the document processing.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot2859">... tokens.</A><A
 HREF="normalization-equivalence-classing-of-terms-1.html#tex2html19"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>It is also often referred to as <A NAME="2265"></A> <I>term normalization</I> ,
  but we prefer to reserve the name <I>term</I> for the output of the
  normalization process.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot2862">...
documents.</A><A
 HREF="normalization-equivalence-classing-of-terms-1.html#tex2html21"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>At the time we wrote this chapter (Aug. 2005), this was actually
   the case on 
   Google: the top result for the query <I>C.A.T.</I> was a site
   about cats, the Cat Fanciers Web Site <TT><A NAME="tex2html22"
  HREF="http://www.fanciers.com/">http://www.fanciers.com/</A></TT>.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot2900">... part-of-speech-tagging.</A><A
 HREF="biword-indexes-1.html#tex2html28"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Part of speech taggers classify words as nouns, verbs, etc. - or,
    in practice, often as finer-grained classes like ``plural proper
    noun''.  Many fairly accurate (c. 96% per-tag accuracy) part-of-speech
    taggers now exist, 
    usually trained by machine learning methods on hand-tagged text.
    See, for instance, <A
 HREF="bibliography-1.html#manning99foundations">Manning and Sch&#252;tze (1999,  ch.&nbsp;10)</A>.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot3634">... maintain.</A><A
 HREF="search-structures-for-dictionaries-1.html#tex2html30"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>So-called perfect hash functions are designed to preclude collisions, but are rather more complicated both to implement and to compute.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot5352">...clusters </A><A
 HREF="distributed-indexing-1.html#tex2html39"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD><A NAME="p:computercluster"></A> A <A NAME="5087"></A>cluster  
in this chapter is
a group of tightly coupled computers that work together
closely. This sense of the word is different from the use of
cluster as a group of
documents that are semantically similar in flatclustlsi.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot5361">... structure</A><A
 HREF="dynamic-indexing-1.html#tex2html42"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>See,
for example, (<A
 HREF="bibliography-1.html#cormen90algorithms">Cormen et&nbsp;al., 1990</A>, Chapter&nbsp;19).

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot6414">... list.</A><A
 HREF="variable-byte-codes-1.html#tex2html54"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Note that the origin
is 0 in the table. Because we never need to encode a docID or
a gap of 0, in practice the origin is usually 1, so that
10000000 encodes 1, 10000101 encodes 6 (not 5 as in the
table), and so on. 


<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot6447">...
removed.</A><A
 HREF="gamma-codes-1.html#tex2html56"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We assume here that <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img284.png"
 ALT="$G$"> has no leading
0s. If there are any, they are
removed  before deleting the leading 1.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot6755">...
distribution</A><A
 HREF="gamma-codes-1.html#tex2html58"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Readers who want to review basic
concepts of probability theory
may want to consult
<A
 HREF="bibliography-1.html#rice06statistics">Rice (2006)</A> or
<A
 HREF="bibliography-1.html#sheldon06probability">Ross (2006)</A>. Note that we are interested in
probability distributions over integers (gaps, frequencies,
etc.), but that the coding properties of a probability
distribution are independent of whether the outcomes are
integers or something else.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot6506">... .</A><A
 HREF="gamma-codes-1.html#tex2html59"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Note that, unfortunately, the conventional symbol for both
entropy and harmonic number is <IMG
 WIDTH="18" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img317.png"
 ALT="$H$">. Context should make
clear which is meant in this chapter.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot10755">... all,</A><A
 HREF="evaluation-of-ranked-retrieval-results-1.html#tex2html79"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>A system may not fully order all documents in the collection in
    response to a query or at any rate an evaluation exercise may be
    based on submitting only the top <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$k$"> results for each information need.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot11024">... agreement.</A><A
 HREF="assessing-relevance-1.html#tex2html82"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>For a contingency table, as in Table <A HREF="assessing-relevance-1.html#tab:kappa">8.2</A> , a marginal statistic
   is formed by summing a 
   row or column.  The marginal <!-- MATH
 $a_{i.k} = \sum_j a_{ijk}$
 -->
<IMG
 WIDTH="89" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img565.png"
 ALT="$a_{i.k} = \sum_j a_{ijk}$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot11025">... need.</A><A
 HREF="results-snippets-1.html#tex2html83"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>There are exceptions, in domains where recall is 
  emphasized. For instance, in many legal disclosure cases, a legal associate
  will review <I>every</I> document that matches a keyword search. 
  

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot11756">... find:</A><A
 HREF="the-underlying-theory-1.html#tex2html89"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>
In the equation, <A NAME="p:argmax"></A> <!-- MATH
 $\argmax_x f(x)$
 -->
<IMG
 WIDTH="101" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img575.png"
 ALT="$\argmax_x f(x)$"> returns a value of <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img58.png"
 ALT="$x$"> which
   maximizes the value of the function <IMG
 WIDTH="35" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img576.png"
 ALT="$f(x)$">.
Similarly, <!-- MATH
 $\argmin_x f(x)$
 -->
<IMG
 WIDTH="99" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img577.png"
 ALT="$\argmin_x f(x)$"> returns a value of <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img58.png"
 ALT="$x$"> which
   minimizes the value of the function <IMG
 WIDTH="35" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img576.png"
 ALT="$f(x)$">.

<P>



<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot12110">... .</A><A
 HREF="xml-retrieval-1.html#tex2html91"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In most modern database
systems, one can enable full-text search for text columns.
This usually means that an inverted index is created and
Boolean or vector space search enabled, effectively
combining core database with information retrieval
technologies.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot13000">... .</A><A
 HREF="basic-xml-concepts-1.html#tex2html94"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>The
  representation is simplified in a number of respects. For
  example, we
  do not show the root node and text is not
  embedded in text nodes. See <TT><A NAME="tex2html95"
  HREF="http://www.w3.org/DOM/">http://www.w3.org/DOM/</A></TT>.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot13018">... war.</A><A
 HREF="basic-xml-concepts-1.html#tex2html98"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>To represent the semantics
  of NEXI queries fully we would also need to designate one node
  in the tree as a ``target node'', for example, the
  section in the tree in Figure <A HREF="basic-xml-concepts-1.html#fig:nexitopic">10.3</A> . Without the
  designation of a target node, the tree in Figure <A HREF="basic-xml-concepts-1.html#fig:nexitopic">10.3</A>  is not
  a search for sections embedded in articles (as specified
  by NEXI), but a search for articles that contain sections.


<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot14463">... hold.</A><A
 HREF="review-of-basic-probability-theory-1.html#tex2html112"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>The term <I>likelihood</I> is just a synonym of
    <I>probability</I>. It is the probability of an event or data
    according to a model.  The term is usually used when people
    are thinking of holding the data fixed, while varying the model.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot15186">...
automaton.</A><A
 HREF="finite-automata-and-language-models-1.html#tex2html114"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Finite automata can have outputs attached to either their
    states or their arcs; we use states here, because that maps
    directly on to the way probabilistic automata are usually formalized.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot15230">... documents.</A><A
 HREF="finite-automata-and-language-models-1.html#tex2html115"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In the IR context that we are leading up to, taking the
   stop probability to be fixed across models seems reasonable.
   This is because we
   are generating queries, and the length distribution of queries is
   fixed and independent of the document from which we are generating
   the language model.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot15596">... collection.</A><A
 HREF="using-query-likelihood-language-models-in-ir-1.html#tex2html117"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>
 Of course, in other cases, they do not.  The answer to this within the 
language modeling approach is <A NAME="15355"></A>translation language models, as briefly discussed in Section <A HREF="extended-language-modeling-approaches-1.html#sec:extended-lm">12.4</A> .

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot15599">... distribution.</A><A
 HREF="estimating-the-query-generation-probability-1.html#tex2html118"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In the context of probability theory, (re)normalization
    refers to summing numbers that cover an event space and dividing
    them through by their sum, so that the result is a probability
    distribution which sums to 1.  This is distinct from both the
    concept of term normalization in Chapter <A HREF="the-term-vocabulary-and-postings-lists-1.html#ch:vocabulary-postings">2</A>  and
    the concept of length normalization in Chapter <A HREF="scoring-term-weighting-and-the-vector-space-model-1.html#ch:tfidf">6</A> , which is
    done with a <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img476.png"
 ALT="$L_2$"> norm.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot15407">... model.</A><A
 HREF="estimating-the-query-generation-probability-1.html#tex2html119"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>It is also referred to as Jelinek-Mercer smoothing.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot16273">....</A><A
 HREF="naive-bayes-text-classification-1.html#tex2html123"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We will explain in the next section
why <IMG
 WIDTH="48" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img868.png"
 ALT="$P(c\vert d)$"> is proportional to (<IMG
 WIDTH="15" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img869.png"
 ALT="$\propto$">), not equal to the quantity on
the right.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot17785">....</A><A
 HREF="naive-bayes-text-classification-1.html#tex2html127"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Our assumption here
  is that the length of test documents is bounded. <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img918.png"
 ALT="$ L_{a}$"> would exceed <IMG
 WIDTH="55" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img922.png"
 ALT="$b\vert C\vert M_{a}$"> for extremely long
test documents.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot16726">...
magnitude.</A><A
 HREF="properties-of-naive-bayes-1.html#tex2html128"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In fact, if the length of documents is not
  bounded, the number of parameters in the multinomial case
  is infinite.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot17810">... symbol.</A><A
 HREF="properties-of-naive-bayes-1.html#tex2html131"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Our terminology is
  nonstandard. The random variable <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img295.png"
 ALT="$X$"> is a
  categorical variable, not a multinomial variable, and the
corresponding  NB model should perhaps be called a
<A NAME="16804"></A>   <A NAME="16805"></A> <I>sequence model</I> . We have chosen to present this sequence model and the multinomial model in Section <A HREF="a-variant-of-the-multinomial-model-1.html#sec:variantmultinomial">13.4.1</A>  as the same
model because they are computationally identical.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot17821">....</A><A
 HREF="mutual-information-1.html#tex2html136"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Take care not to confuse expected mutual
information with <A NAME="16994"></A><I>pointwise mutual information</I>, which is
defined as <!-- MATH
 $\log \observationo_{11}/E_{11}$
 -->
<IMG
 WIDTH="87" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img1008.png"
 ALT="$\log \observationo_{11}/E_{11}$"> where 
<!-- MATH
 $\observationo_{11}$
 -->
<IMG
 WIDTH="29" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1009.png"
 ALT="$\observationo_{11}$"> and <IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1010.png"
 ALT="$E_{11}$"> are defined as in 
Equation&nbsp;<A HREF="feature-selectionchi2-feature-selection-1.html#eqn:chisquare">133</A>.
The two
measures have different properties. See
Section <A HREF="references-and-further-reading-13.html#sec:further2">13.7</A> .

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot17858">... .</A><A
 HREF="mutual-information-1.html#tex2html138"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Feature scores were computed on the first
100,000 documents, except for poultry, a rare class, for
which 800,000 documents were used. We have omitted
numbers and other special words from the top
ten lists.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot17224">...
Reuters-RCV1.</A><A
 HREF="mutual-information-1.html#tex2html140"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We trained the classifiers on the first
100,000 documents and computed <IMG
 WIDTH="19" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img522.png"
 ALT="$F_1$"> on the next
100,000. The graphs are averages over five classes.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot17916">... wrong.</A><A
 HREF="feature-selectionchi2-feature-selection-1.html#tex2html141"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We can make this
inference because,
if the two events are
independent, then <!-- MATH
 $X^{\kern.5pt2} \sim \chi^2$
 -->
<IMG
 WIDTH="63" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img1064.png"
 ALT="$X^{\kern.5pt2} \sim \chi^2$">, where <IMG
 WIDTH="21" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img21.png"
 ALT="$\chi ^2$"> is the
<IMG
 WIDTH="21" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img21.png"
 ALT="$\chi ^2$"> distribution. See, for example,
<A NAME="17872"></A> <A
 HREF="bibliography-1.html#rice06statistics">Rice (2006)</A>.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot20826">...
vector</A><A
 HREF="rocchio-classification-1.html#tex2html151"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Recall from basic linear algebra that
<!-- MATH
 $\vec{v} \cdot \vec{w} = \vec{v}^{T} \vec{w}$
 -->
<IMG
 WIDTH="86" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img1100.png"
 ALT="$\vec{v} \cdot \vec{w} = \vec{v}^{T} \vec{w}$">, i.e., the
dot product of <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img433.png"
 ALT="$\vec{v}$"> and <IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1099.png"
 ALT="$\vec{w}$"> equals the product
by matrix multiplication of the transpose of <IMG
 WIDTH="12" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img433.png"
 ALT="$\vec{v}$"> and
<IMG
 WIDTH="16" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1099.png"
 ALT="$\vec{w}$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot20838">...
classification.</A><A
 HREF="rocchio-classification-1.html#tex2html154"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We
write 
<!-- MATH
 $\Theta(|\docsetlabeled| L_{ave})$
 -->
<IMG
 WIDTH="83" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img914.png"
 ALT="$\Theta(\vert\docsetlabeled\vert L_{ave})$">
for <IMG
 WIDTH="41" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img124.png"
 ALT="$\Theta(T)$"> 
and assume
that the length of
  test
  documents is bounded as we did on
  page <A HREF="naive-bayes-text-classification-1.html#p:dlentestdvoctestsimplification">13.2</A> .


<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot20852">... ).</A><A
 HREF="k-nearest-neighbor-1.html#tex2html155"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>
The generalization of a polygon to higher dimensions is a
<A NAME="20173"></A> <I>polytope</I> . A polytope is a region in <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img186.png"
 ALT="$M$">-dimensional space
bounded by <IMG
 WIDTH="61" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img1137.png"
 ALT="$(M-1)$">-dimensional hyperplanes.
In <IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img186.png"
 ALT="$M$"> dimensions, the decision boundaries for kNN 
consist of segments of 
<IMG
 WIDTH="61" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img1137.png"
 ALT="$(M-1)$">-dimensional hyperplanes that form
the Voronoi tessellation into convex polytopes for the training set
of documents. The decision criterion of assigning a document
to the majority class of its <IMG
 WIDTH="11" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img20.png"
 ALT="$k$"> nearest neighbors
applies equally to <IMG
 WIDTH="50" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1138.png"
 ALT="$M=2$"> (tessellation into polygons) and <IMG
 WIDTH="50" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1139.png"
 ALT="$M&gt;2$">
(tessellation into polytopes).


<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot20867">...polytomous </A><A
 HREF="classification-with-more-than-two-classes-1.html#tex2html159"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>A synonym
of polytomous is <A NAME="20497"></A>polychotomous.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot23206">... it.</A><A
 HREF="support-vector-machines-the-linearly-separable-case-1.html#tex2html163"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>As discussed in Section&nbsp;<A HREF="document-representations-and-measures-of-relatedness-in-vector-spaces-1.html#sec:simdisfigs">14.1</A> (page&nbsp;<A HREF="document-representations-and-measures-of-relatedness-in-vector-spaces-1.html#p:simdisfigs"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/crossref.png"></A>), we present the general case of points
  in a vector space, but if the points are length normalized document vectors,
  then all the action is taking place on the surface of a unit sphere,
  and the decision surface intersects the sphere's surface.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot23229">... gives:</A><A
 HREF="support-vector-machines-the-linearly-separable-case-1.html#tex2html165"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Recall that <!-- MATH
 $|\vec{w}| = \sqrt{\vec{w}^\mathrm{T}\vec{w}}$
 -->
<IMG
 WIDTH="94" HEIGHT="40" ALIGN="MIDDLE" BORDER="0"
 SRC="img1281.png"
 ALT="$\vert\vec{w}\vert = \sqrt{\vec{w}^\mathrm{T}\vec{w}}$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot23220">... .</A><A
 HREF="soft-margin-classification-1.html#tex2html169"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We
write 
<!-- MATH
 $\Theta(|\docsetlabeled| L_{ave})$
 -->
<IMG
 WIDTH="83" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img914.png"
 ALT="$\Theta(\vert\docsetlabeled\vert L_{ave})$">
for <IMG
 WIDTH="41" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img124.png"
 ALT="$\Theta(T)$"> (page <A HREF="naive-bayes-text-classification-1.html#p:dlenavetheta">13.2</A> ) and assume
that the length of
  test
  documents is bounded as we did on
  page <A HREF="naive-bayes-text-classification-1.html#p:dlentestdvoctestsimplification">13.2</A> .


<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot22709">...
SVM\@.</A><A
 HREF="soft-margin-classification-1.html#tex2html170"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Materializing the features refers to directly calculating higher
   order and interaction terms and then putting them into a linear model.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot23223">... .</A><A
 HREF="experimental-results-1.html#tex2html172"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>These results are in terms of the <A NAME="22895"></A>break-even <IMG
 WIDTH="19" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img522.png"
 ALT="$F_1$"> 
   (see Section <A HREF="evaluation-of-ranked-retrieval-results-1.html#sec:ranked-evaluation">8.4</A> ).  Many researchers disprefer 
   this measure for text classification evaluation, since its
   calculation may involve interpolation rather than an actual
   parameter setting of the system and it is not clear why this value
   should be reported rather than maximal <IMG
 WIDTH="19" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img522.png"
 ALT="$F_1$"> or another point on
   the precision/recall curve motivated by the task at hand.
   While earlier results in (<A
 HREF="bibliography-1.html#joachims98text">Joachims, 1998</A>) suggested notable gains 
   on this task from the use
   of higher order polynomial or rbf kernels, this was with
   hard-margin SVMs.  With soft-margin SVMs, a simple linear SVM with
   the default <IMG
 WIDTH="45" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1359.png"
 ALT="$C=1$"> performs best.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot23225">... modest.</A><A
 HREF="large-and-difficult-category-taxonomies-1.html#tex2html173"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Using the small hierarchy in Figure&nbsp;<A HREF="the-text-classification-problem-1.html#fig:setupstatclass">13.1</A> (page&nbsp;<A HREF="the-text-classification-problem-1.html#p:setupstatclass"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/crossref.png"></A>) as an
   example, the leaf classes are ones like <I>poultry</I> and
   <I>coffee</I>, as opposed to higher-up classes like <I>industries</I>.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot25224">...
feasible.</A><A
 HREF="cardinality---the-number-of-clusters-1.html#tex2html180"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>An upper bound on the number of
  clusterings is <IMG
 WIDTH="52" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img1392.png"
 ALT="$K^N/K!$">.  The
exact number of different partitions of <IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.png"
 ALT="$N$"> documents into
<IMG
 WIDTH="15" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img30.png"
 ALT="$K$"> clusters is the Stirling number of the second kind.  See
<TT><A NAME="tex2html181"
  HREF="http://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html">http://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html</A></TT>
or <A
 HREF="bibliography-1.html#comtet74advanced">Comtet (1974)</A>.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot25225">... .</A><A
 HREF="evaluation-of-clustering-1.html#tex2html182"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Recall our note of caution from
Figure <A HREF="document-representations-and-measures-of-relatedness-in-vector-spaces-1.html#fig:3dsphere">14.2</A>  (page <A HREF="document-representations-and-measures-of-relatedness-in-vector-spaces-1.html#p:3dsphere">14.2</A> ) when looking at
this and other 2D figures in this and the following
chapter: these illustrations
can be misleading because 2D projections of length-normalized
vectors distort
similarities and distances between points.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot25265">...
model.</A><A
 HREF="model-based-clustering-1.html#tex2html184"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD><IMG
 WIDTH="26" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1492.png"
 ALT="$\wvar_m$"> is the random variable we
defined in Section <A HREF="the-bernoulli-model-1.html#sec:twomodels">13.3</A>  (page <A HREF="properties-of-naive-bayes-1.html#p:wvar">13.4</A> ) for the
Bernoulli Naive Bayes model. It takes the values 1 (term
<IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1493.png"
 ALT="$\tcword_m$"> is present in the document) and 0 (term
<IMG
 WIDTH="20" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1493.png"
 ALT="$\tcword_m$"> is absent in the document).

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot25272">... EM.</A><A
 HREF="model-based-clustering-1.html#tex2html185"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>For example, this problem is common when EM is used to
  estimate parameters of
hidden Markov models, probabilistic grammars, and machine
translation models 
<A NAME="25056"></A>in natural
language processing
(<A
 HREF="bibliography-1.html#manning99foundations">Manning and Sch&#252;tze, 1999</A>).

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot27283">...
clustering.</A><A
 HREF="hierarchical-clustering-1.html#tex2html187"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>In this chapter, we only consider
  hierarchies<A NAME="26354"></A>
that are
<A NAME="26355"></A>binary
trees like the one shown in Figure <A HREF="hierarchical-agglomerative-clustering-1.html#fig:rprojectsingle">17.1</A>  - but hierarchical
clustering can be easily extended to other types of trees.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot26482">... updated.</A><A
 HREF="hierarchical-agglomerative-clustering-1.html#tex2html189"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>We assume
  that we use a deterministic method for breaking ties,
  such as always choose the merge that is the first cluster with respect to a
  total ordering of the subsets of the document set
  <IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img1484.png"
 ALT="$D$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot26529">... (a))</A><A
 HREF="single-link-and-complete-link-clustering-1.html#tex2html190"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Throughout this chapter, we equate
  similarity with proximity in 2D depictions of clustering.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot26545">...
similarity.</A><A
 HREF="single-link-and-complete-link-clustering-1.html#tex2html191"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>If you are bothered by the possibility
of ties, assume that <IMG
 WIDTH="19" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img412.png"
 ALT="$d_1$"> has coordinates
<!-- MATH
 $(1+\epsilon,3-\epsilon)$
 -->
<IMG
 WIDTH="95" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img1563.png"
 ALT="$(1+\epsilon,3-\epsilon)$"> and that all other points have
integer coordinates.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot27317">...
labeling.</A><A
 HREF="cluster-labeling-1.html#tex2html193"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Selecting the most frequent terms is 
a non-differential 
feature selection technique we discussed in Section <A HREF="feature-selection-1.html#sec:feature">13.5</A> .
It can also be used for labeling clusters.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot30621">... 2.1.</A><A
 HREF="the-web-graph-1.html#tex2html201"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Cf. Zipf's law of the distribution of words in text in Chapter <A HREF="index-compression-1.html#ch:icompress">5</A>  (page <A HREF="zipfs-law-modeling-the-distribution-of-terms-1.html#p:zipfnew">5.2</A> ), which is a power law with <IMG
 WIDTH="43" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img589.png"
 ALT="$\alpha = 1$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot31546">... host</A><A
 HREF="the-url-frontier-1.html#tex2html213"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>The number of hosts is assumed to far exceed <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img168.png"
 ALT="$B$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot31766">... cluster</A><A
 HREF="distributing-indexes-1.html#tex2html215"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Please note the different usage of ``clusters'' elsewhere in this book, in the sense of Chapters <A HREF="flat-clustering-1.html#ch:flatclust">16</A> <A HREF="hierarchical-clustering-1.html#ch:hierclust">17</A> .

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot32245">... graph</A><A
 HREF="pagerank-1.html#tex2html223"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>This is consistent with our usage of <IMG
 WIDTH="17" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.png"
 ALT="$N$"> for the number of documents in the collection.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
<DT><A NAME="foot32353">...<IMG
 WIDTH="28" HEIGHT="35" ALIGN="MIDDLE" BORDER="0"
 SRC="img1917.png"
 ALT="$\vec{x}P^t$"></A><A
 HREF="the-pagerank-computation-1.html#tex2html225"><SUP><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]"
 SRC="../icons/footnote.png"></SUP></A></DT>
<DD>Note that <IMG
 WIDTH="19" HEIGHT="35" ALIGN="MIDDLE" BORDER="0"
 SRC="img1918.png"
 ALT="$P^t$"> represents <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"> raised to the <IMG
 WIDTH="10" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img67.png"
 ALT="$t$">th power, not the transpose of <IMG
 WIDTH="14" HEIGHT="32" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.png"
 ALT="$P$"> which is denoted <IMG
 WIDTH="23" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img1919.png"
 ALT="$P^T$">.

<PRE>.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
</PRE>
</DD>
</DL>
</BODY>

<!-- Mirrored from nlp.stanford.edu/IR-book/html/htmledition/footnode.html by HTTrack Website Copier/3.x [XR&CO'2013], Sat, 08 Feb 2014 19:40:24 GMT -->
</HTML>
